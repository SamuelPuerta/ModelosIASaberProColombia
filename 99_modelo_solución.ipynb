{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCIJO_awQ40B"
      },
      "source": [
        "# **PROYECTO KAGGLE - Pruebas Saber Pro Colombia**\n",
        "\n",
        "Modelo Predictivo a partir de la tabla de datos train.csv sobre el rendimiento de los estudiantes en las pruebas Saber Pro.\n",
        "\n",
        "Hecho por:\n",
        "- Juan Manuel Areiza Ospina - C.C. 1018226898\n",
        "- Samuel Puerta Patiño - C.C. 1023624795\n",
        "- Brayan Stiven Gómez Villa - C.C 1018224235"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAbEHy7HPsMa"
      },
      "source": [
        "\n",
        "\n",
        "Se implementa un modelo de clasificación para predecir el rendimiento global de estudiantes en las Pruebas Saber Pro de Colombia, utilizando XGBoost con optimización de hiperparámetros mediante Optuna y pseudo-etiquetado.\n",
        "\n",
        "El contexto del proyecto es predecir el desempeño de estudiantes (bajo, medio-bajo, medio-alto, alto) basado en datos socioeconómicos, institucionales y estadísticos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwAlff1KPsMb"
      },
      "source": [
        "## Importaciones\n",
        "\n",
        "En esta sección se importan las bibliotecas necesarias para el procesamiento de datos, modelado y optimización. Pandas y NumPy para manejo de datos, Optuna para optimización de hiperparámetros, XGBoost como modelo de clasificación, y herramientas de scikit-learn para validación cruzada y preprocesamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhQaK4sWQGD4",
        "outputId": "1a7e7977-95cb-49eb-e68e-990d7d0341c2"
      },
      "outputs": [],
      "source": [
        "pip install pandas numpy optuna xgboost scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EhIEC8MvPsMc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\puert\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pandas.api.types import CategoricalDtype\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ycc5ed4pR5jF",
        "outputId": "196f9adb-6bc5-49c7-b94d-19d9ba2752ca"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '.'\n",
        "!chmod 600 ./kaggle.json\n",
        "!kaggle competitions download -c udea-ai-4-eng-20252-pruebas-saber-pro-colombia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Duph5K9R59x",
        "outputId": "12957650-42d5-4825-cc67-17edf53104db"
      },
      "outputs": [],
      "source": [
        "!unzip udea-ai-4-eng-20252-pruebas-saber-pro-colombia.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0q7Voh0PsMc"
      },
      "source": [
        "## Configuración\n",
        "\n",
        "Aquí se definen las constantes y configuraciones del script, como los nombres de archivos, la columna objetivo, el estado aleatorio para reproducibilidad, el número de pruebas de Optuna y el umbral para pseudo-etiquetado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f10eZP9QPsMd"
      },
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "TRAIN_FILE = \"train.csv\"\n",
        "TEST_FILE = \"test.csv\"\n",
        "SUBMISSION_FILE = \"submission_bestmodel.csv\"\n",
        "TARGET_COL = \"RENDIMIENTO_GLOBAL\"\n",
        "RANDOM_STATE = 42\n",
        "N_TRIALS = 10  # Increased to 20\n",
        "PSEUDO_LABEL_THRESHOLD = 0.90 # Back to 0.90 for safety"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yGtmUxGPsMd"
      },
      "source": [
        "## Funciones Auxiliares\n",
        "\n",
        "Estas funciones ayudan en la carga de datos, limpieza de texto y ingeniería de características. Son cruciales para preparar los datos antes del modelado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SusPpyxbPsMd"
      },
      "source": [
        "### Función load_data\n",
        "\n",
        "Esta función carga los datos de entrenamiento y prueba desde los archivos CSV especificados. Verifica que el archivo de entrenamiento exista y devuelve los DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7a2hnFbfPsMd"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    print(\"Loading data...\")\n",
        "    if not os.path.exists(TRAIN_FILE):\n",
        "        raise FileNotFoundError(f\"Train file not found at {os.path.abspath(TRAIN_FILE)}\")\n",
        "\n",
        "    train_df = pd.read_csv(TRAIN_FILE)\n",
        "    test_df = pd.read_csv(TEST_FILE)\n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9RwkhJZPsMe"
      },
      "source": [
        "### Función clean_text\n",
        "\n",
        "Esta función limpia y estandariza las columnas de texto en el DataFrame. Elimina espacios en blanco, estandariza respuestas Sí/No y corrige inconsistencias en el texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DRJqe42OPsMe"
      },
      "outputs": [],
      "source": [
        "def clean_text(df):\n",
        "    # Standardize text columns\n",
        "    df.columns = df.columns.str.strip()\n",
        "    # Map all string columns to strip whitespace\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "    # Standardize Yes/No\n",
        "    replace_dict = {\n",
        "        \"Si\": \"Sí\", \"si\": \"Sí\", \"sÃ­\": \"Sí\",\n",
        "        \"N\": \"No\", \"NO\": \"No\", \"n\": \"No\"\n",
        "    }\n",
        "    df.replace(replace_dict, inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPnAOZagPsMe"
      },
      "source": [
        "### Función feature_engineering\n",
        "\n",
        "Esta función realiza la ingeniería de características: elimina la columna ID, maneja la alta cardinalidad en programas académicos, codifica ordinalmente variables categóricas, crea índices socioeconómicos y de educación parental, y calcula presión financiera. También elimina columnas ruidosas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSgEqPs_PsMe",
        "outputId": "702b86d9-3a76-474c-cb97-6ac19a7e7114"
      },
      "outputs": [],
      "source": [
        "def feature_engineering(df, is_train=True, allowed_programs=None):\n",
        "    print(\"Engineering features...\")\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1. Drop ID\n",
        "    if 'ID' in df.columns:\n",
        "        df = df.drop(columns=['ID'])\n",
        "\n",
        "    # 2. Handle E_PRGM_ACADEMICO (High Cardinality)\n",
        "    if 'E_PRGM_ACADEMICO' in df.columns:\n",
        "        if is_train:\n",
        "            frecuencias = df[\"E_PRGM_ACADEMICO\"].value_counts()\n",
        "            allowed_programs = set(frecuencias.head(200).index) # Increased to 250 to match best model\n",
        "            df[\"E_PRGM_ACADEMICO\"] = df[\"E_PRGM_ACADEMICO\"].apply(lambda x: x if x in allowed_programs else \"OTROS_PROGRAMAS\")\n",
        "        else:\n",
        "            if allowed_programs is not None:\n",
        "                df[\"E_PRGM_ACADEMICO\"] = df[\"E_PRGM_ACADEMICO\"].apply(lambda x: x if x in allowed_programs else \"OTROS_PROGRAMAS\")\n",
        "\n",
        "    # 3. Ordinal Encoding & Numeric Conversion\n",
        "\n",
        "    # Hours Worked\n",
        "    orden_horas = ['0', \"Menos de 10 horas\", \"Entre 11 y 20 horas\", \"Entre 21 y 30 horas\", \"Más de 30 horas\"]\n",
        "    horas_map = {k: i for i, k in enumerate(orden_horas)}\n",
        "    if \"E_HORASSEMANATRABAJA\" in df.columns:\n",
        "        df[\"E_HORASSEMANATRABAJA_NUM\"] = df[\"E_HORASSEMANATRABAJA\"].map(horas_map).fillna(0)\n",
        "\n",
        "    # Education Level\n",
        "    orden_educacion = [\n",
        "        \"Ninguno\", \"No sabe\", \"No Aplica\", \"Primaria incompleta\", \"Primaria completa\",\n",
        "        \"Secundaria (Bachillerato) incompleta\", \"Secundaria (Bachillerato) completa\",\n",
        "        \"Técnica o tecnológica incompleta\", \"Técnica o tecnológica completa\",\n",
        "        \"Educación profesional incompleta\", \"Educación profesional completa\", \"Postgrado\"\n",
        "    ]\n",
        "    # Custom mapping to give \"Postgrado\" higher value\n",
        "    edu_map = {k: i for i, k in enumerate(orden_educacion)}\n",
        "\n",
        "    for col in [\"F_EDUCACIONPADRE\", \"F_EDUCACIONMADRE\"]:\n",
        "        if col in df.columns:\n",
        "            df[f\"{col}_NUM\"] = df[col].map(edu_map).fillna(-1)\n",
        "\n",
        "    # Estrato\n",
        "    if \"F_ESTRATOVIVIENDA\" in df.columns:\n",
        "        # Extract number from \"Estrato 1\", etc.\n",
        "        df[\"F_ESTRATOVIVIENDA_NUM\"] = df[\"F_ESTRATOVIVIENDA\"].astype(str).str.extract('(\\d+)').astype(float).fillna(0)\n",
        "\n",
        "    # 4. Interaction Features\n",
        "\n",
        "    # Socioeconomic Index\n",
        "    # Map Yes/No to 1/0\n",
        "    yes_no_cols = ['F_TIENEINTERNET', 'F_TIENELAVADORA', 'F_TIENEAUTOMOVIL', 'F_TIENECOMPUTADOR']\n",
        "    for col in yes_no_cols:\n",
        "        if col in df.columns:\n",
        "            df[f\"{col}_NUM\"] = df[col].apply(lambda x: 1 if x == 'Sí' else 0)\n",
        "\n",
        "    # Combine duplicates if F_TIENEINTERNET.1 exists\n",
        "    if 'F_TIENEINTERNET.1' in df.columns:\n",
        "        df['F_TIENEINTERNET_COMBINED'] = df.apply(lambda row: 1 if (row.get('F_TIENEINTERNET') == 'Sí' or row.get('F_TIENEINTERNET.1') == 'Sí') else 0, axis=1)\n",
        "    elif 'F_TIENEINTERNET' in df.columns:\n",
        "        df['F_TIENEINTERNET_COMBINED'] = df['F_TIENEINTERNET_NUM']\n",
        "    else:\n",
        "        df['F_TIENEINTERNET_COMBINED'] = 0\n",
        "\n",
        "    # Create Index\n",
        "    df['SOCIOECONOMIC_INDEX'] = (\n",
        "        df.get('F_ESTRATOVIVIENDA_NUM', 0) +\n",
        "        df.get('F_TIENELAVADORA_NUM', 0) +\n",
        "        df.get('F_TIENEAUTOMOVIL_NUM', 0) +\n",
        "        df.get('F_TIENECOMPUTADOR_NUM', 0) +\n",
        "        df.get('F_TIENEINTERNET_COMBINED', 0)\n",
        "    )\n",
        "\n",
        "    # Parental Education Index\n",
        "    df['PARENT_EDU_INDEX'] = (df.get('F_EDUCACIONPADRE_NUM', 0) + df.get('F_EDUCACIONMADRE_NUM', 0)) / 2\n",
        "\n",
        "    # Financial Pressure Proxy (Tuition * Work Hours)\n",
        "    # Matricula is usually categorical ranges, let's map roughly\n",
        "    matricula_map = {\n",
        "        \"No pagó matrícula\": 0,\n",
        "        \"Menos de 500 mil\": 250000,\n",
        "        \"Entre 500 mil y menos de 1 millón\": 750000,\n",
        "        \"Entre 1 millón y menos de 2.5 millones\": 1750000,\n",
        "        \"Entre 2.5 millones y menos de 4 millones\": 3250000,\n",
        "        \"Entre 4 millones y menos de 5.5 millones\": 4750000,\n",
        "        \"Entre 5.5 millones y menos de 7 millones\": 6250000,\n",
        "        \"Más de 7 millones\": 8000000\n",
        "    }\n",
        "    if \"E_VALORMATRICULAUNIVERSIDAD\" in df.columns:\n",
        "        df[\"VALOR_MATRICULA_EST\"] = df[\"E_VALORMATRICULAUNIVERSIDAD\"].map(matricula_map).fillna(0)\n",
        "        df[\"FINANCIAL_PRESSURE\"] = df[\"VALOR_MATRICULA_EST\"] * df.get(\"E_HORASSEMANATRABAJA_NUM\", 0)\n",
        "\n",
        "    # 5. DROP NOISY COLUMNS (Crucial Step from Best Model)\n",
        "    # We used these to create indices, now we drop the raw ones that were deemed noisy\n",
        "    cols_to_drop = [\n",
        "        'F_TIENELAVADORA', 'F_TIENEAUTOMOVIL',\n",
        "        'E_PRIVADO_LIBERTAD', 'F_TIENEINTERNET.1',\n",
        "        'INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4',\n",
        "        # Also drop the intermediate numeric cols we created if they are redundant,\n",
        "        # but let's keep the _NUM ones as they are better than raw text.\n",
        "        # We drop the original text ones if we have _NUM\n",
        "    ]\n",
        "    # Actually, let's strictly follow the best model's drop list + our used raw inputs\n",
        "    # Best model dropped: 'E_VALORMATRICULAUNIVERSIDAD', 'F_TIENELAVADORA', 'F_TIENEAUTOMOVIL', 'E_PRIVADO_LIBERTAD', 'E_PAGOMATRICULAPROPIO', 'F_TIENEINTERNET.1', 'INDICADOR_1'...'INDICADOR_4'\n",
        "\n",
        "    final_drop = [c for c in cols_to_drop if c in df.columns]\n",
        "    df = df.drop(columns=final_drop)\n",
        "\n",
        "    return df, allowed_programs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukm02r6iPsMe"
      },
      "source": [
        "## Función Principal (main)\n",
        "\n",
        "Esta es la función principal que ejecuta todo el flujo: carga los datos, los limpia, realiza ingeniería de características, prepara los datos para el modelo, optimiza hiperparámetros con Optuna, aplica pseudo-etiquetado, hace predicciones finales y guarda los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibXA8KX5PsMe"
      },
      "source": [
        "### Optimización con Optuna\n",
        "\n",
        "Se define una función objetivo para Optuna que prueba diferentes hiperparámetros del modelo XGBoost usando validación cruzada estratificada. El objetivo es maximizar la precisión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg0KgRdbPsMf"
      },
      "source": [
        "### Pseudo-Etiquetado\n",
        "\n",
        "Se entrena un modelo inicial con los mejores parámetros, se predicen probabilidades en el conjunto de prueba, y se seleccionan muestras de alta confianza para agregar al conjunto de entrenamiento, mejorando el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3uNWtP4PsMf"
      },
      "source": [
        "### Predicción Final y Guardado\n",
        "\n",
        "Se generan las predicciones finales en el conjunto de prueba, se decodifican las etiquetas, se crea el archivo de envío y se muestra la importancia de las características."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DslfosNpPsMf",
        "outputId": "df677a10-dd6d-4aeb-9b66-a4d59a9133f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Engineering features...\n",
            "Engineering features...\n",
            "Categorical columns: ['E_PRGM_ACADEMICO', 'E_PRGM_DEPARTAMENTO', 'E_VALORMATRICULAUNIVERSIDAD', 'E_HORASSEMANATRABAJA', 'F_ESTRATOVIVIENDA', 'F_TIENEINTERNET', 'F_EDUCACIONPADRE', 'E_PAGOMATRICULAPROPIO', 'F_TIENECOMPUTADOR', 'F_EDUCACIONMADRE']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-27 21:42:28,698] A new study created in memory with name: no-name-97b3595b-bc93-418d-94e5-4a7381a2e6e2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Optuna with 10 trials...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-27 21:45:32,945] Trial 0 finished with value: 0.4370555939305875 and parameters: {'learning_rate': 0.05187350821660059, 'max_depth': 4, 'subsample': 0.7664558168271524, 'colsample_bytree': 0.9963090323253676, 'reg_alpha': 3.25783313718403, 'reg_lambda': 3.9030385638655414, 'n_estimators': 596, 'min_child_weight': 3}. Best is trial 0 with value: 0.4370555939305875.\n",
            "[I 2025-11-27 21:48:58,380] Trial 1 finished with value: 0.4356938618709943 and parameters: {'learning_rate': 0.03587671858786245, 'max_depth': 4, 'subsample': 0.6286975317224421, 'colsample_bytree': 0.9230514746749398, 'reg_alpha': 4.389410559745184, 'reg_lambda': 3.8175492637868462, 'n_estimators': 592, 'min_child_weight': 5}. Best is trial 0 with value: 0.4370555939305875.\n",
            "[I 2025-11-27 21:51:10,028] Trial 2 finished with value: 0.4379523448894203 and parameters: {'learning_rate': 0.1412263776184059, 'max_depth': 6, 'subsample': 0.6157847655233132, 'colsample_bytree': 0.782103212008406, 'reg_alpha': 4.486706240380768, 'reg_lambda': 4.237234407778203, 'n_estimators': 237, 'min_child_weight': 3}. Best is trial 2 with value: 0.4379523448894203.\n",
            "[I 2025-11-27 21:56:17,701] Trial 3 finished with value: 0.43887075674830367 and parameters: {'learning_rate': 0.06281409665709381, 'max_depth': 7, 'subsample': 0.7281301013237216, 'colsample_bytree': 0.6584617066980667, 'reg_alpha': 4.561900640201102, 'reg_lambda': 1.390219020699765, 'n_estimators': 481, 'min_child_weight': 2}. Best is trial 3 with value: 0.43887075674830367.\n",
            "[I 2025-11-27 21:59:50,590] Trial 4 finished with value: 0.43561732637594175 and parameters: {'learning_rate': 0.15560352912336245, 'max_depth': 7, 'subsample': 0.7515607178783809, 'colsample_bytree': 0.6540827526100261, 'reg_alpha': 2.9635649818429135, 'reg_lambda': 0.4617922758028089, 'n_estimators': 341, 'min_child_weight': 1}. Best is trial 3 with value: 0.43887075674830367.\n",
            "[I 2025-11-27 22:03:05,161] Trial 5 finished with value: 0.43158555962697415 and parameters: {'learning_rate': 0.18868616058729107, 'max_depth': 8, 'subsample': 0.7902406978949733, 'colsample_bytree': 0.7475430684866553, 'reg_alpha': 2.530928323274628, 'reg_lambda': 2.6574022416399856, 'n_estimators': 276, 'min_child_weight': 4}. Best is trial 3 with value: 0.43887075674830367.\n",
            "[I 2025-11-27 22:08:05,650] Trial 6 finished with value: 0.4385906124398922 and parameters: {'learning_rate': 0.030228559453252854, 'max_depth': 8, 'subsample': 0.8912281070904756, 'colsample_bytree': 0.9076168926681099, 'reg_alpha': 4.1004783668310365, 'reg_lambda': 1.6844605550104594, 'n_estimators': 436, 'min_child_weight': 4}. Best is trial 3 with value: 0.43887075674830367.\n",
            "[I 2025-11-27 22:14:01,726] Trial 7 finished with value: 0.43312057584666547 and parameters: {'learning_rate': 0.10828722118990189, 'max_depth': 8, 'subsample': 0.6486219712923088, 'colsample_bytree': 0.9254709651313289, 'reg_alpha': 3.837505851416584, 'reg_lambda': 1.606998883313742, 'n_estimators': 475, 'min_child_weight': 1}. Best is trial 3 with value: 0.43887075674830367.\n",
            "[I 2025-11-27 22:22:04,198] Trial 8 finished with value: 0.4212953055097155 and parameters: {'learning_rate': 0.19451245378193566, 'max_depth': 9, 'subsample': 0.8986718351260854, 'colsample_bytree': 0.7304566011347965, 'reg_alpha': 3.605515972355629, 'reg_lambda': 3.4806032266599267, 'n_estimators': 597, 'min_child_weight': 4}. Best is trial 3 with value: 0.43887075674830367.\n",
            "[I 2025-11-27 22:27:51,205] Trial 9 finished with value: 0.41899205571367154 and parameters: {'learning_rate': 0.18843127374664312, 'max_depth': 10, 'subsample': 0.956715091551115, 'colsample_bytree': 0.8356930787814307, 'reg_alpha': 1.412185854097625, 'reg_lambda': 2.905305085279904, 'n_estimators': 381, 'min_child_weight': 2}. Best is trial 3 with value: 0.43887075674830367.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best CV Accuracy: 0.4389\n",
            "\n",
            "--- Pseudo-Labeling Phase ---\n",
            "Found 2401 pseudo-labels with confidence > 0.9\n",
            "Retraining with augmented size: 694901\n",
            "Generating final predictions...\n",
            "Saved submission_bestmodel.csv\n",
            "                        Feature  Importance\n",
            "1              E_PRGM_ACADEMICO    0.153550\n",
            "3   E_VALORMATRICULAUNIVERSIDAD    0.129922\n",
            "22          VALOR_MATRICULA_EST    0.127947\n",
            "2           E_PRGM_DEPARTAMENTO    0.064393\n",
            "20          SOCIOECONOMIC_INDEX    0.048776\n",
            "8         E_PAGOMATRICULAPROPIO    0.048496\n",
            "21             PARENT_EDU_INDEX    0.045191\n",
            "0             PERIODO_ACADEMICO    0.035155\n",
            "10             F_EDUCACIONMADRE    0.032130\n",
            "18        F_TIENECOMPUTADOR_NUM    0.030175\n",
            "23           FINANCIAL_PRESSURE    0.030118\n",
            "13         F_EDUCACIONMADRE_NUM    0.027513\n",
            "5             F_ESTRATOVIVIENDA    0.027374\n",
            "7              F_EDUCACIONPADRE    0.027212\n",
            "4          E_HORASSEMANATRABAJA    0.026204\n",
            "14        F_ESTRATOVIVIENDA_NUM    0.023547\n",
            "9             F_TIENECOMPUTADOR    0.022893\n",
            "6               F_TIENEINTERNET    0.021206\n",
            "11     E_HORASSEMANATRABAJA_NUM    0.017338\n",
            "15          F_TIENEINTERNET_NUM    0.015784\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # 1. Load\n",
        "    train_df_raw, test_df_raw = load_data()\n",
        "    test_ids = test_df_raw['ID']\n",
        "\n",
        "    # 2. Clean & Preprocess\n",
        "    train_df_clean = clean_text(train_df_raw)\n",
        "    test_df_clean = clean_text(test_df_raw)\n",
        "\n",
        "    train_df, allowed_programs = feature_engineering(train_df_clean, is_train=True)\n",
        "    test_df, _ = feature_engineering(test_df_clean, is_train=False, allowed_programs=allowed_programs)\n",
        "\n",
        "        # Prepare X, y\n",
        "    X = train_df.drop(columns=[TARGET_COL], errors='ignore')\n",
        "    y_raw = train_df[TARGET_COL]\n",
        "    X_test = test_df.drop(columns=['ID'], errors='ignore')\n",
        "\n",
        "    # Align columns\n",
        "    missing_cols = set(X.columns) - set(X_test.columns)\n",
        "    for c in missing_cols:\n",
        "        X_test[c] = 0\n",
        "    X_test = X_test[X.columns]\n",
        "\n",
        "    # Encode Target\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_raw)\n",
        "\n",
        "    # Convert object columns to category for XGBoost\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "    print(f\"Categorical columns: {categorical_cols}\")\n",
        "    for col in categorical_cols:\n",
        "        X[col] = X[col].astype(\"category\")\n",
        "        X_test[col] = X_test[col].astype(\"category\")\n",
        "\n",
        "    # 3. Optuna Optimization\n",
        "    def objective(trial):\n",
        "        param = {\n",
        "            'objective': 'multi:softmax',\n",
        "            'num_class': len(le.classes_),\n",
        "            'tree_method': 'hist',\n",
        "            'enable_categorical': True,\n",
        "            'eval_metric': 'mlogloss',\n",
        "            'random_state': RANDOM_STATE,\n",
        "            'n_jobs': -1,\n",
        "\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
        "            'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 5),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 5),\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 5)\n",
        "        }\n",
        "\n",
        "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
        "        model = xgb.XGBClassifier(**param)\n",
        "        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "        return scores.mean()\n",
        "\n",
        "    print(f\"Starting Optuna with {N_TRIALS} trials...\")\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=N_TRIALS)\n",
        "\n",
        "    print(f\"Best CV Accuracy: {study.best_value:.4f}\")\n",
        "    best_params = study.best_params\n",
        "\n",
        "    # 4. Pseudo-Labeling\n",
        "    print(\"\\n--- Pseudo-Labeling Phase ---\")\n",
        "\n",
        "    # Train initial best model\n",
        "    model_params = best_params.copy()\n",
        "    model_params.update({\n",
        "        'objective': 'multi:softmax',\n",
        "        'num_class': len(le.classes_),\n",
        "        'tree_method': 'hist',\n",
        "        'enable_categorical': True,\n",
        "        'random_state': RANDOM_STATE,\n",
        "        'n_jobs': -1\n",
        "    })\n",
        "\n",
        "    initial_model = xgb.XGBClassifier(**model_params)\n",
        "    initial_model.fit(X, y)\n",
        "\n",
        "    # Predict probabilities on test\n",
        "    probs = initial_model.predict_proba(X_test)\n",
        "    max_probs = np.max(probs, axis=1)\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    # Select high confidence samples\n",
        "    high_conf_indices = np.where(max_probs > PSEUDO_LABEL_THRESHOLD)[0]\n",
        "    print(f\"Found {len(high_conf_indices)} pseudo-labels with confidence > {PSEUDO_LABEL_THRESHOLD}\")\n",
        "\n",
        "    if len(high_conf_indices) > 0:\n",
        "        X_pseudo = X_test.iloc[high_conf_indices]\n",
        "        y_pseudo = preds[high_conf_indices]\n",
        "\n",
        "        # Combine with original train\n",
        "        X_augmented = pd.concat([X, X_pseudo], axis=0)\n",
        "        y_augmented = np.concatenate([y, y_pseudo], axis=0)\n",
        "\n",
        "        print(f\"Retraining with augmented size: {len(X_augmented)}\")\n",
        "        final_model = xgb.XGBClassifier(**model_params)\n",
        "        final_model.fit(X_augmented, y_augmented)\n",
        "    else:\n",
        "        print(\"No pseudo-labels added. Using initial model.\")\n",
        "        final_model = initial_model\n",
        "\n",
        "        # 5. Final Prediction\n",
        "    print(\"Generating final predictions...\")\n",
        "    final_preds = final_model.predict(X_test)\n",
        "    final_preds_decoded = le.inverse_transform(final_preds)\n",
        "\n",
        "    submission = pd.DataFrame({'ID': test_ids, 'RENDIMIENTO_GLOBAL': final_preds_decoded})\n",
        "    submission.to_csv(SUBMISSION_FILE, index=False)\n",
        "    print(f\"Saved {SUBMISSION_FILE}\")\n",
        "\n",
        "    # Feature Importance\n",
        "    importance = final_model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "    fi_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "    print(fi_df.sort_values(by='Importance', ascending=False).head(20))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
